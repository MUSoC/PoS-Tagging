from nltk.tokenize import word_tokenize as tokenize
from tokenizer import word_tokenize
from time import time
from os import stat

def file_size(f): #Function returns file size in bytes
    fileInfo = stat(f)
    return fileInfo.st_size

def speed(nltkTime,tokenizerTime): #Function returns a tuple which includes the difference in time taken
    if nltkTime<tokenizerTime:
        return ("NLTK","tokenizer",abs(nltkTime-tokenizerTime))
    else:
        return ("tokenizer","NLTK",abs(nltkTime-tokenizerTime))
 
def num_tokens(nltkTokens,tokenizerTokens): #Function returns a tuple which includes the difference in number of tokens
    nltk_num = len(nltkTokens)
    tokenizer_num = len([j for i in tokenizerTokens for j in i])
    if nltk_num<tokenizer_num:
        return ("tokenizer",abs(nltk_num-tokenizer_num),"NLTK")
    else:
        return ("NLTK",abs(nltk_num-tokenizer_num),"tokenizer")

print ("Size of Brown corpus in bytes:  ",file_size("brown.txt"))
text = open("brown.txt","r").read() #Read the Brown corpus
t0 = time()
nltkTokens = tokenize(text) #Tokenizer with NLTK
t1 = time() 
nltkTime = t1-t0
print ("Time taken by NLTK's word_tokenize to tokenize text: ",nltkTime)
print ("Number of tokens generated by NLTK's word_tokenize: ",len(nltkTokens))
t2 = time()
tokenizerTokens = word_tokenize(text) #Tokenizer with tokenizer
t3 = time()
tokenizerTime = t3-t2
print ("Time taken by tokenizer's word_tokenize to tokenize text: ",tokenizerTime)
print ("Number of tokens generated by tokenizer's word_tokenize: ",len([j for i in tokenizerTokens for j in i]))
functionSpeed = speed(nltkTime,tokenizerTime)
print (functionSpeed[0],"is faster than",functionSpeed[1],"by",functionSpeed[2],"seconds")
numberOfTokens = num_tokens(nltkTokens,tokenizerTokens)
print (numberOfTokens[0],"generated",numberOfTokens[1],"more tokens than",numberOfTokens[2])
